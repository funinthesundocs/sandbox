---
phase: 02-scraping-pipeline-4-hours
plan: 03
type: execute
wave: 2
depends_on:
  - "02-01"
  - "02-02"
files_modified:
  - src/worker/handlers/scrape.ts
  - src/worker/index.ts
  - src/app/api/remix-engine/scrape/route.ts
  - src/app/api/remix-engine/scrape/batch/route.ts
  - src/app/api/remix-engine/scrape/preview/route.ts
  - src/app/api/remix-engine/channel/route.ts
autonomous: true
requirements:
  - R2.5
  - R2.7
  - R2.8
  - R2.9
  - R2.6

must_haves:
  truths:
    - "handleScrapeJob() runs download + metadata in parallel, extracts transcript, uploads to Supabase Storage, updates re_videos and re_jobs — all in try/finally with cleanTempDir"
    - "cleanTempDir(videoId) is called in the finally block of handleScrapeJob regardless of success or failure"
    - "All storage uploads use storagePath() helper — never manual path construction"
    - "re_jobs progress updated at each pipeline step: 0 → 10 (start) → 40 (download+metadata done) → 60 (transcript done) → 85 (storage upload done) → 100 (complete)"
    - "POST /api/remix-engine/scrape enqueues scrape job, returns { jobId, videoId } — validated with ScrapeRequestSchema"
    - "POST /api/remix-engine/scrape/preview returns metadata preview without downloading"
    - "Duplicate detection returns { existing: true, videoId } before enqueueing"
    - "src/worker/index.ts registers the scrape Worker with concurrency: 3"
  artifacts:
    - path: "src/worker/handlers/scrape.ts"
      provides: "handleScrapeJob(job: Job) — complete scrape pipeline orchestrator"
      exports: ["handleScrapeJob"]
    - path: "src/worker/index.ts"
      provides: "Worker entry point with scrape Worker registered, concurrency 3, graceful shutdown"
    - path: "src/app/api/remix-engine/scrape/route.ts"
      provides: "POST handler — enqueue scrape job with duplicate detection"
    - path: "src/app/api/remix-engine/scrape/batch/route.ts"
      provides: "POST handler — enqueue up to 10 videos as batch"
    - path: "src/app/api/remix-engine/scrape/preview/route.ts"
      provides: "POST handler — fetch metadata preview without downloading"
    - path: "src/app/api/remix-engine/channel/route.ts"
      provides: "GET handler — list channel videos for browse-and-pick UI"
  key_links:
    - from: "src/worker/handlers/scrape.ts"
      to: "src/lib/scraper/temp-files.ts"
      via: "cleanTempDir(videoId) in finally block — MANDATORY"
      pattern: "cleanTempDir"
    - from: "src/worker/handlers/scrape.ts"
      to: "src/lib/supabase/admin.ts"
      via: "supabaseAdmin for all DB operations (service role — bypasses RLS)"
      pattern: "supabaseAdmin"
    - from: "src/worker/handlers/scrape.ts"
      to: "src/lib/remix-engine/hooks.ts"
      via: "storagePath() for all storage path construction"
      pattern: "storagePath"
    - from: "src/app/api/remix-engine/scrape/route.ts"
      to: "src/lib/queue/queues.ts"
      via: "scrapeQueue.add() to enqueue job"
      pattern: "scrapeQueue"
    - from: "src/worker/index.ts"
      to: "src/worker/handlers/scrape.ts"
      via: "handleScrapeJob passed to Worker constructor"
      pattern: "handleScrapeJob"
---

<objective>
Wire the scraper library (Plan 01) and YouTube API client (Plan 02) into the BullMQ worker handler and Next.js API routes. This is the integration layer: the worker handler orchestrates the full pipeline, and the API routes enqueue jobs and serve the preview/channel browse endpoints.

Purpose: Without this plan, the scraper library has no entry point and no way to be triggered. After this plan, pasting a YouTube URL into the (future) UI will enqueue a job that downloads, transcribes, and uploads the video.
Output: Complete worker handler + five API routes covering single scrape, batch scrape, preview, and channel browse.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/02-scraping-pipeline-4-hours/02-RESEARCH.md

CRITICAL RULES:
- Worker handler: RELATIVE IMPORTS ONLY (no @/ aliases) — uses tsconfig.worker.json
- Worker uses supabaseAdmin (service role) — not the cookie-based client
- cleanTempDir(videoId) MUST be in finally block — non-negotiable
- storagePath() helper for ALL storage paths — never construct manually
- Scrape concurrency: 3 (BullMQ Worker concurrency option)
- Progress update pattern: update re_jobs.progress at each step AND call job.updateProgress()
- STORAGE BUCKET: 'remix-engine' (from Phase 1 migration 003_storage_setup.sql)

STORAGE PATH STRUCTURE (using storagePath() helper):
- Video: storagePath('videos', projectId, videoId, 'original.mp4')
- Thumbnail: storagePath('videos', projectId, videoId, 'thumbnail.jpg')
- Transcript (JSON segments): storagePath('videos', projectId, videoId, 'transcript.json')
- Transcript (plain text): storagePath('videos', projectId, videoId, 'transcript.txt')

API ROUTE AUTH PATTERN: Import createClient from '@/lib/supabase/server'. Call const supabase = await createClient(). const { data: { user } } = await supabase.auth.getUser(). If !user → return NextResponse.json({ error: 'Unauthorized' }, { status: 401 }).

RE_JOBS TABLE COLUMNS (from types.ts):
- id, type ('scrape'|'scrape_batch'|...), status ('queued'|'processing'|'complete'|'error'|'cancelled')
- video_id, project_id, progress (0-100), result (json), error_message
- created_by, started_at, completed_at, created_at

RE_VIDEOS TABLE KEY COLUMNS:
- youtube_url, youtube_id, project_id, batch_job_id
- original_title, original_description, original_thumbnail_url, original_transcript
- channel_name, channel_id, duration_seconds, view_count, published_at
- video_file_path, thumbnail_file_path, transcript_file_path
- scrape_status ('pending'|'processing'|'complete'|'error'), error_message

<interfaces>
<!-- From Plan 01 — import via relative path in worker, @/ alias in Next.js routes -->
import { downloadYouTubeVideo } from '../../lib/scraper/youtube-downloader';
import { extractSubtitles } from '../../lib/scraper/transcript-extractor';
import { parseVTT, transcriptToPlainText } from '../../lib/scraper/vtt-parser';
import { cleanTempDir } from '../../lib/scraper/temp-files';
import { ScrapeError } from '../../lib/scraper/error-codes';

<!-- From Plan 02 — relative path -->
import { fetchVideoMetadata } from '../../lib/youtube-api/metadata';
import { parseYouTubeUrl, extractYouTubeId } from '../../lib/youtube-api/url-parser';

<!-- From Phase 1 — relative path (worker context) -->
import { supabaseAdmin } from '../../lib/supabase/admin';
import { storagePath } from '../../lib/remix-engine/hooks';
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: BullMQ scrape worker handler</name>
  <files>
    src/worker/handlers/scrape.ts
    src/worker/index.ts
  </files>
  <action>
**src/worker/handlers/scrape.ts** — The full scrape pipeline orchestrator. ALL imports must be relative (no @/ aliases). This file runs in the worker process.

Imports (relative):
```typescript
import { Job } from 'bullmq';
import fs from 'fs';
import { downloadYouTubeVideo } from '../../lib/scraper/youtube-downloader';
import { extractSubtitles } from '../../lib/scraper/transcript-extractor';
import { parseVTT, transcriptToPlainText, TranscriptSegment } from '../../lib/scraper/vtt-parser';
import { cleanTempDir } from '../../lib/scraper/temp-files';
import { ScrapeError, ScrapeErrorCode } from '../../lib/scraper/error-codes';
import { fetchVideoMetadata } from '../../lib/youtube-api/metadata';
import { supabaseAdmin } from '../../lib/supabase/admin';
import { storagePath } from '../../lib/remix-engine/hooks';
```

Export interface ScrapeJobData:
```typescript
export interface ScrapeJobData {
  youtubeUrl: string;
  youtubeId: string;
  projectId: string;
  videoId: string;    // UUID — pre-generated by API route when creating re_videos record
  userId: string;     // Created-by user ID
  batchJobId?: string; // Present for batch jobs
}
```

Export `handleScrapeJob(job: Job<ScrapeJobData>): Promise<{ success: true; videoId: string }>`:

**MANDATORY PATTERN** — try/finally with cleanTempDir:
```typescript
try {
  // all work here
} catch (error) {
  // error handling + re_jobs update
  throw error; // re-throw so BullMQ marks job as failed
} finally {
  cleanTempDir(job.data.videoId); // ALWAYS runs — disk cleanup
}
```

**Pipeline steps inside try:**

Step 0 — Mark job as processing:
```typescript
await supabaseAdmin
  .from('re_jobs')
  .update({ status: 'processing', started_at: new Date().toISOString(), progress: 10 })
  .eq('id', job.id);
await supabaseAdmin
  .from('re_videos')
  .update({ scrape_status: 'processing' })
  .eq('id', videoId);
await job.updateProgress(10);
```

Step 1 — Download video + fetch metadata in parallel (both I/O-bound, run simultaneously):
```typescript
const [downloadResult, metadata] = await Promise.all([
  downloadYouTubeVideo(youtubeUrl, videoId),
  fetchVideoMetadata(youtubeId),
]);
// Update progress to 40
await updateProgress(40);
```

Step 2 — Extract subtitles (sequential — needs the temp dir from download):
```typescript
const vttPath = await extractSubtitles(youtubeUrl, videoId);
let transcriptSegments: TranscriptSegment[] = [];
let transcriptText = '';
if (vttPath) {
  transcriptSegments = parseVTT(vttPath);
  transcriptText = transcriptToPlainText(transcriptSegments);
}
// If vttPath is null: transcriptSegments stays [], transcriptText stays ''
// This is not an error — job continues with no transcript
await updateProgress(60);
```

Step 3 — Upload assets to Supabase Storage (parallel uploads):
```typescript
const bucket = 'remix-engine';
const videoPath = storagePath('videos', projectId, videoId, 'original.mp4');
const thumbnailPath = storagePath('videos', projectId, videoId, 'thumbnail.jpg');
const transcriptJsonPath = storagePath('videos', projectId, videoId, 'transcript.json');
const transcriptTxtPath = storagePath('videos', projectId, videoId, 'transcript.txt');

// Fetch thumbnail from URL (metadata.thumbnailUrl is a public YouTube CDN URL)
const thumbnailBuffer = Buffer.from(
  await fetch(metadata.thumbnailUrl).then(r => r.arrayBuffer())
);

const uploads = await Promise.all([
  supabaseAdmin.storage.from(bucket).upload(videoPath, fs.readFileSync(downloadResult.filePath), {
    contentType: 'video/mp4', upsert: true
  }),
  supabaseAdmin.storage.from(bucket).upload(thumbnailPath, thumbnailBuffer, {
    contentType: 'image/jpeg', upsert: true
  }),
  supabaseAdmin.storage.from(bucket).upload(transcriptJsonPath,
    Buffer.from(JSON.stringify(transcriptSegments)), {
    contentType: 'application/json', upsert: true
  }),
  supabaseAdmin.storage.from(bucket).upload(transcriptTxtPath,
    Buffer.from(transcriptText), {
    contentType: 'text/plain', upsert: true
  }),
]);

// Check for upload errors
const uploadErrors = uploads.filter(u => u.error);
if (uploadErrors.length > 0) {
  throw new ScrapeError('STORAGE_UPLOAD_FAILED', 'Failed to upload assets to storage.');
}
await updateProgress(85);
```

Step 4 — Update re_videos record with all scraped data:
```typescript
const { error: videoError } = await supabaseAdmin
  .from('re_videos')
  .update({
    original_title: metadata.title,
    original_description: metadata.description,
    original_thumbnail_url: metadata.thumbnailUrl,
    original_transcript: transcriptText || null,
    channel_name: metadata.channelName,
    channel_id: metadata.channelId,
    duration_seconds: metadata.durationSeconds,
    view_count: metadata.viewCount,
    published_at: metadata.publishedAt,
    video_file_path: videoPath,
    thumbnail_file_path: thumbnailPath,
    transcript_file_path: transcriptJsonPath,
    scrape_status: 'complete',
    error_message: null,
  })
  .eq('id', videoId);
```

Step 5 — Mark re_jobs complete:
```typescript
await supabaseAdmin
  .from('re_jobs')
  .update({
    status: 'complete',
    progress: 100,
    completed_at: new Date().toISOString(),
    result: { videoId, transcriptSegmentCount: transcriptSegments.length },
  })
  .eq('id', job.id);
await job.updateProgress(100);
return { success: true, videoId };
```

**Catch block** — Map ScrapeError to user-facing message, update re_jobs and re_videos with error:
```typescript
catch (error: unknown) {
  const userMessage = error instanceof ScrapeError
    ? error.userMessage
    : 'An unexpected error occurred during scraping.';

  await supabaseAdmin.from('re_jobs').update({
    status: 'error',
    error_message: userMessage,
    completed_at: new Date().toISOString(),
  }).eq('id', job.id);

  await supabaseAdmin.from('re_videos').update({
    scrape_status: 'error',
    error_message: userMessage,
  }).eq('id', videoId);

  throw error; // Re-throw so BullMQ marks job as failed
}
```

Create a local helper `updateProgress(pct: number)` that calls both job.updateProgress(pct) AND supabaseAdmin.from('re_jobs').update({ progress: pct }).eq('id', job.id) — DRY.

**src/worker/index.ts** — UPDATE the existing stub to register the scrape Worker.

REPLACE the existing stub entirely. Keep the relative import constraint. Add Worker import from bullmq:
```typescript
import { Worker } from 'bullmq';
import IORedis from 'ioredis';
import { handleScrapeJob } from './handlers/scrape';
```

Get Redis URL from process.env.REDIS_URL (worker process reads env directly — documented exception):
```typescript
const redisConnection = new IORedis(process.env.REDIS_URL || 'redis://localhost:6379', {
  maxRetriesPerRequest: null,
});
```

Register workers:
```typescript
const scrapeWorker = new Worker('scrape', handleScrapeJob, {
  connection: redisConnection,
  concurrency: 3,
});

scrapeWorker.on('completed', (job) => {
  console.log(`Scrape job ${job.id} completed for video ${job.data?.videoId}`);
});

scrapeWorker.on('failed', (job, err) => {
  console.error(`Scrape job ${job?.id} failed:`, err.message);
});
```

Graceful shutdown:
```typescript
const shutdown = async (signal: string) => {
  console.log(`Worker received ${signal}, shutting down...`);
  await scrapeWorker.close();
  await redisConnection.quit();
  process.exit(0);
};
process.on('SIGTERM', () => shutdown('SIGTERM'));
process.on('SIGINT', () => shutdown('SIGINT'));
```
  </action>
  <verify>
    <automated>cd /c/Antigravity/Sandbox && npx tsc -p tsconfig.worker.json --noEmit 2>&1 | head -30</automated>
  </verify>
  <done>
    Worker TypeScript compiles with tsconfig.worker.json (no @/ alias errors). handleScrapeJob exported from handlers/scrape.ts. cleanTempDir called in finally block. storagePath() used for all storage paths. re_jobs progress updated at steps 10/40/60/85/100. Worker index.ts registers scrapeWorker with concurrency: 3.
  </done>
</task>

<task type="auto">
  <name>Task 2: Scrape API routes (enqueue, preview, batch, channel browse)</name>
  <files>
    src/app/api/remix-engine/scrape/route.ts
    src/app/api/remix-engine/scrape/preview/route.ts
    src/app/api/remix-engine/scrape/batch/route.ts
    src/app/api/remix-engine/channel/route.ts
  </files>
  <action>
These are Next.js API routes. Use @/ imports. All routes require authentication.

**AUTH PATTERN** for all routes:
```typescript
import { createClient } from '@/lib/supabase/server';
const supabase = await createClient();
const { data: { user } } = await supabase.auth.getUser();
if (!user) return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
```

**src/app/api/remix-engine/scrape/route.ts** — POST: enqueue a single video scrape.

Imports: createClient from '@/lib/supabase/server', createAdminClient from '@/lib/supabase/admin', scrapeQueue from '@/lib/queue/queues', ScrapeRequestSchema from '@/lib/validators/schemas', extractYouTubeId from '@/lib/youtube-api/url-parser', { randomUUID } from 'crypto', NextResponse from 'next/server'.

POST handler logic:
1. Auth check (pattern above)
2. Parse body: const body = await request.json(). Validate: const result = ScrapeRequestSchema.safeParse(body). If !result.success → 400 with result.error.flatten()
3. const { youtubeUrl, projectId } = result.data
4. const youtubeId = extractYouTubeId(youtubeUrl) — throws if not a video URL → catch → 400
5. **Duplicate detection**: Check if re_videos already has this youtubeId + projectId:
```typescript
const { data: existing } = await supabaseAdmin
  .from('re_videos')
  .select('id, scrape_status')
  .eq('youtube_id', youtubeId)
  .eq('project_id', projectId)
  .single();
if (existing) {
  return NextResponse.json({ existing: true, videoId: existing.id, scrapeStatus: existing.scrape_status }, { status: 200 });
}
```
6. Create re_videos record (status: pending):
```typescript
const videoId = randomUUID();
await supabaseAdmin.from('re_videos').insert({
  id: videoId,
  project_id: projectId,
  youtube_url: youtubeUrl,
  youtube_id: youtubeId,
  scrape_status: 'pending',
  created_by: user.id,
});
```
7. Create re_jobs record:
```typescript
const jobId = randomUUID();
await supabaseAdmin.from('re_jobs').insert({
  id: jobId,
  type: 'scrape',
  status: 'queued',
  video_id: videoId,
  project_id: projectId,
  progress: 0,
  created_by: user.id,
});
```
8. Enqueue BullMQ job:
```typescript
await scrapeQueue.add('scrape', {
  youtubeUrl,
  youtubeId,
  projectId,
  videoId,
  userId: user.id,
}, { jobId });
```
9. Return NextResponse.json({ jobId, videoId }, { status: 201 })

**src/app/api/remix-engine/scrape/preview/route.ts** — POST: fetch metadata preview without downloading.

Body: { youtubeUrl: string, projectId: string }

1. Auth check
2. Validate youtubeUrl (use z.string().url() inline or reuse ScrapeRequestSchema.shape.youtubeUrl)
3. extractYouTubeId(youtubeUrl) — get youtubeId
4. const metadata = await fetchVideoMetadata(youtubeId) — from '@/lib/youtube-api/metadata'
5. Check for duplicate: same as scrape route, return existing: true if found
6. Check duration: if metadata.durationSeconds > 20 * 60 → return 422 { error: 'TOO_LONG', message: `Video is ${Math.round(metadata.durationSeconds/60)} minutes. Maximum is 20 minutes.` }
7. Return NextResponse.json({ preview: { youtubeId, title: metadata.title, thumbnailUrl: metadata.thumbnailUrl, durationSeconds: metadata.durationSeconds, channelName: metadata.channelName, viewCount: metadata.viewCount }, existing: false })

**src/app/api/remix-engine/scrape/batch/route.ts** — POST: enqueue up to 10 video URLs at once.

Body: { youtubeUrls: string[], projectId: string } — validate: youtubeUrls is array, max 10 items.

1. Auth check
2. Validate body: youtubeUrls is string[], length 1-10, projectId is uuid
3. For each URL: extractYouTubeId, check duplicate, create re_videos record, create re_jobs record, add to scrapeQueue. Collect results.
4. Return { enqueued: [{ videoId, jobId, youtubeId }], duplicates: [{ videoId, youtubeId }], errors: [{ url, error }] }

Process URLs sequentially (not in parallel) to avoid overwhelming the DB with inserts. This is a small batch (max 10).

**src/app/api/remix-engine/channel/route.ts** — GET: list channel videos for browse-and-pick UI.

Query params: channelUrl (string), pageToken (string, optional)

1. Auth check
2. Get channelUrl from searchParams. If missing → 400.
3. Parse it: const parsed = parseYouTubeUrl(channelUrl). If type === 'video' → 400 { error: 'URL is a video, not a channel' }
4. const channelId = await resolveChannelId(parsed.id) — from '@/lib/youtube-api/channel'
5. const page = await fetchChannelVideos(channelId, pageToken || undefined)
6. Return NextResponse.json({ channelId, items: page.items, nextPageToken: page.nextPageToken, totalResults: page.totalResults })
  </action>
  <verify>
    <automated>cd /c/Antigravity/Sandbox && npx tsc --noEmit 2>&1 | head -30</automated>
  </verify>
  <done>
    TypeScript compiles cleanly. All four API routes exist. POST /api/remix-engine/scrape creates re_videos + re_jobs records and enqueues BullMQ job. Preview route returns metadata without downloading. Batch route handles up to 10 URLs. Channel route returns paginated video list. Duplicate detection returns existing: true with videoId.
  </done>
</task>

</tasks>

<verification>
Run: npx tsc --noEmit — must pass with zero errors.
Run: npx tsc -p tsconfig.worker.json --noEmit — must pass with zero errors.
Check: grep "cleanTempDir" src/worker/handlers/scrape.ts — must appear in finally block.
Check: grep "storagePath" src/worker/handlers/scrape.ts — must appear for all storage uploads.
Check: grep "@/" src/worker/handlers/scrape.ts — must return empty (worker uses relative imports).
Check: src/app/api/remix-engine/channel/route.ts exports GET function.
</verification>

<success_criteria>
- handleScrapeJob orchestrates: download + metadata parallel → transcript → storage upload → DB update
- cleanTempDir in finally block (verified by grep)
- All storage paths via storagePath() helper (verified by grep)
- Worker TypeScript compiles with tsconfig.worker.json (no @/ aliases)
- scrapeWorker registered with concurrency: 3
- POST /api/remix-engine/scrape detects duplicates before creating records
- POST /api/remix-engine/scrape/preview returns preview without downloading
- GET /api/remix-engine/channel returns paginated channel video list
</success_criteria>

<output>
After completion, create `.planning/phases/02-scraping-pipeline-4-hours/02-03-SUMMARY.md` following the summary template.
</output>
