---
phase: 02-scraping-pipeline-4-hours
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/scraper/youtube-downloader.ts
  - src/lib/scraper/transcript-extractor.ts
  - src/lib/scraper/vtt-parser.ts
  - src/lib/scraper/video-utils.ts
  - src/lib/scraper/temp-files.ts
  - src/lib/scraper/error-codes.ts
autonomous: true
requirements:
  - R2.1
  - R2.3
  - R2.4
  - R2.8
  - R2.9

must_haves:
  truths:
    - "downloadYouTubeVideo() wraps yt-dlp with 5-minute timeout, compresses >200MB files to 720p, rejects >20-min videos"
    - "extractSubtitles() runs yt-dlp --write-auto-sub and returns vttPath or null if no captions"
    - "parseVTT() returns TranscriptSegment[] with timestamp string, startMs number, and clean text — deduplicates adjacent identical lines"
    - "cleanTempDir() deletes /tmp/remixengine/{videoId}/ and is always called in finally blocks"
    - "Error detection maps yt-dlp stderr to PRIVATE_VIDEO | AGE_RESTRICTED | UNAVAILABLE | DOWNLOAD_TIMEOUT | TOO_LONG error codes"
    - "All files use relative imports only (no @/ aliases) — worker-safe"
  artifacts:
    - path: "src/lib/scraper/youtube-downloader.ts"
      provides: "downloadYouTubeVideo(url, videoId) → DownloadResult with filePath, duration, sizeBytes"
      exports: ["downloadYouTubeVideo", "DownloadResult"]
    - path: "src/lib/scraper/transcript-extractor.ts"
      provides: "extractSubtitles(url, videoId) → string | null (vttFilePath)"
      exports: ["extractSubtitles"]
    - path: "src/lib/scraper/vtt-parser.ts"
      provides: "parseVTT(filePath) → TranscriptSegment[], TranscriptSegment type"
      exports: ["parseVTT", "TranscriptSegment"]
    - path: "src/lib/scraper/video-utils.ts"
      provides: "getVideoDuration(filePath) → number (seconds), compressTo720p(filePath) → Promise<void>"
      exports: ["getVideoDuration", "compressTo720p"]
    - path: "src/lib/scraper/temp-files.ts"
      provides: "getTempDir(videoId), ensureTempDir(videoId), cleanTempDir(videoId)"
      exports: ["getTempDir", "ensureTempDir", "cleanTempDir"]
    - path: "src/lib/scraper/error-codes.ts"
      provides: "ScrapeError class with code enum, mapYtDlpError() mapper function"
      exports: ["ScrapeError", "ScrapeErrorCode", "mapYtDlpError"]
  key_links:
    - from: "src/lib/scraper/youtube-downloader.ts"
      to: "src/lib/scraper/temp-files.ts"
      via: "getTempDir(videoId) for output directory"
      pattern: "getTempDir"
    - from: "src/lib/scraper/youtube-downloader.ts"
      to: "src/lib/scraper/video-utils.ts"
      via: "getVideoDuration() + compressTo720p() after download"
      pattern: "getVideoDuration"
    - from: "src/lib/scraper/youtube-downloader.ts"
      to: "src/lib/scraper/error-codes.ts"
      via: "mapYtDlpError() in catch block"
      pattern: "mapYtDlpError"
    - from: "src/lib/scraper/transcript-extractor.ts"
      to: "src/lib/scraper/temp-files.ts"
      via: "getTempDir(videoId) for subtitle output"
      pattern: "getTempDir"
---

<objective>
Build the core scraper library — the reusable functions that yt-dlp-wrap video download, subtitle extraction, VTT parsing, temp file management, and error classification. This is pure library code with no external HTTP dependencies; it runs inside the BullMQ worker in Plan 03.

Purpose: Plan 03 (worker handler) imports directly from this library. Plan 04 (API routes) never touches yt-dlp directly. All scraping logic lives here, tested in isolation.
Output: Six library files covering the complete scraper layer.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/02-scraping-pipeline-4-hours/02-RESEARCH.md

CRITICAL RULES (from CLAUDE.md — non-negotiable):
- Worker files use RELATIVE IMPORTS ONLY — no @/ aliases. These files are consumed by both the Next.js app (via @/ alias) AND the worker (via relative path). Use relative imports within src/lib/scraper/*.
- /tmp cleanup MANDATORY in finally blocks — cleanTempDir(videoId) must be called even if job fails
- Max video duration: 20 minutes (1200 seconds). Reject with TOO_LONG error before downloading if duration check fails.
- Compress >200MB files to 720p using ffmpeg before storing
- NEVER use YouTube Captions API — yt-dlp auto-subtitles only (--write-auto-sub --sub-lang en)

SYSTEM DEPENDENCIES (must be installed on the machine running this):
- yt-dlp (via pip3 install yt-dlp)
- ffmpeg (via brew install ffmpeg / apt-get install ffmpeg)

IMPORTANT NOTE ON ZOD: package.json shows zod v4.3.6. Use z.record(z.string(), z.any()) for record types. Use standard Zod v4 API.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Temp file utilities, error codes, video utilities</name>
  <files>
    src/lib/scraper/temp-files.ts
    src/lib/scraper/error-codes.ts
    src/lib/scraper/video-utils.ts
  </files>
  <action>
**src/lib/scraper/temp-files.ts** — Temp directory management. Use relative imports only (import fs from 'fs', import path from 'path').

Export three functions:
- `getTempDir(videoId: string): string` — Returns `/tmp/remixengine/${videoId}` (never construct this path manually elsewhere — always call this)
- `ensureTempDir(videoId: string): string` — Calls getTempDir, then fs.mkdirSync(dir, { recursive: true }), returns the dir path
- `cleanTempDir(videoId: string): void` — Calls fs.rmSync(getTempDir(videoId), { recursive: true, force: true }). Swallows errors (if dir doesn't exist, that's fine — log but don't throw)

Add a JSDoc comment above cleanTempDir: "MANDATORY: Always call in finally block of worker handlers — even on error."

**src/lib/scraper/error-codes.ts** — Typed error handling for scraper failures.

Export a const enum (use `const` + type union, not TypeScript enum — avoid enum transpilation issues):
```typescript
export type ScrapeErrorCode =
  | 'PRIVATE_VIDEO'
  | 'AGE_RESTRICTED'
  | 'UNAVAILABLE'
  | 'DOWNLOAD_TIMEOUT'
  | 'TOO_LONG'
  | 'NO_CAPTIONS'
  | 'STORAGE_UPLOAD_FAILED'
  | 'METADATA_FETCH_FAILED'
  | 'UNKNOWN';
```

Export a `ScrapeError` class extending Error:
```typescript
export class ScrapeError extends Error {
  constructor(
    public readonly code: ScrapeErrorCode,
    public readonly userMessage: string,
    cause?: unknown
  ) {
    super(userMessage, { cause });
    this.name = 'ScrapeError';
  }
}
```

Export `mapYtDlpError(error: unknown): ScrapeError` — inspects error.message (string) for:
- 'Private video' or 'private video' → PRIVATE_VIDEO, "This video is private and cannot be scraped."
- 'age-restricted' or 'age restricted' → AGE_RESTRICTED, "This video is age-restricted."
- 'not available' or 'unavailable' or 'removed' → UNAVAILABLE, "This video is unavailable."
- error.code === 'ETIMEDOUT' or 'ESOCKETTIMEDOUT' → DOWNLOAD_TIMEOUT, "Download timed out. Please try again."
- Falls through to UNKNOWN, "An unexpected error occurred during scraping."

Export `USER_FACING_ERRORS: Record<ScrapeErrorCode, string>` map (same messages as above, for display without instantiating ScrapeError).

**src/lib/scraper/video-utils.ts** — FFmpeg utilities. Import { exec } from 'child_process', { promisify } from 'util', fs from 'fs'.

const execAsync = promisify(exec);

Export `getVideoDuration(filePath: string): Promise<number>` — Run ffprobe:
```
ffprobe -v error -show_entries format=duration -of csv=p=0 "{filePath}"
```
Parse the stdout as a float (seconds). Return it. If ffprobe fails, throw new Error('Failed to get video duration').

Export `compressTo720p(filePath: string): Promise<void>` — Run ffmpeg to re-encode at 720p:
```
ffmpeg -i "{filePath}" -vf scale=-2:720 -c:v libx264 -crf 23 -preset fast -c:a aac -b:a 128k -y "{filePath}.compressed.mp4"
```
After successful compression, fs.renameSync(`${filePath}.compressed.mp4`, filePath). If the compressed file is larger than the original (edge case), keep original and delete compressed. Log both file sizes.

Export `getFileSizeBytes(filePath: string): number` — Returns fs.statSync(filePath).size.
  </action>
  <verify>
    <automated>cd /c/Antigravity/Sandbox && npx tsc --noEmit 2>&1 | head -20</automated>
  </verify>
  <done>
    TypeScript compiles cleanly. getTempDir('abc') returns '/tmp/remixengine/abc'. cleanTempDir has JSDoc comment. ScrapeError class exported. mapYtDlpError('Private video') returns PRIVATE_VIDEO ScrapeError. getVideoDuration and compressTo720p exported from video-utils.ts.
  </done>
</task>

<task type="auto">
  <name>Task 2: yt-dlp download wrapper, subtitle extractor, VTT parser</name>
  <files>
    src/lib/scraper/youtube-downloader.ts
    src/lib/scraper/transcript-extractor.ts
    src/lib/scraper/vtt-parser.ts
  </files>
  <action>
**src/lib/scraper/youtube-downloader.ts** — yt-dlp download wrapper.

Imports: { exec } from 'child_process', { promisify } from 'util', fs from 'fs', path from 'path'.
Import from relative paths: { getTempDir, ensureTempDir } from './temp-files', { getVideoDuration, compressTo720p, getFileSizeBytes } from './video-utils', { ScrapeError, mapYtDlpError } from './error-codes'.

const execAsync = promisify(exec);
const MAX_DURATION_SECONDS = 20 * 60; // 20 minutes
const MAX_FILE_SIZE_BYTES = 200 * 1024 * 1024; // 200MB
const DOWNLOAD_TIMEOUT_MS = 5 * 60 * 1000; // 5 minutes

Export interface DownloadResult:
```typescript
export interface DownloadResult {
  filePath: string;      // Absolute path to the .mp4 file in /tmp
  duration: number;      // Duration in seconds
  sizeBytes: number;     // Final file size after optional compression
}
```

Export `downloadYouTubeVideo(youtubeUrl: string, videoId: string): Promise<DownloadResult>`:

1. Call ensureTempDir(videoId) — create /tmp/remixengine/{videoId}/
2. const outputTemplate = path.join(getTempDir(videoId), '%(id)s.%(ext)s')
3. Build yt-dlp command:
```
yt-dlp -f "bestvideo[height<=1080][ext=mp4]+bestaudio[ext=m4a]/best[height<=1080][ext=mp4]/best" --merge-output-format mp4 --output "${outputTemplate}" --socket-timeout 30 --no-playlist "${youtubeUrl}"
```
4. execAsync(cmd, { timeout: DOWNLOAD_TIMEOUT_MS, maxBuffer: 10 * 1024 * 1024 })
5. Find the output file: glob for *.mp4 in getTempDir(videoId). Use fs.readdirSync(dir).filter(f => f.endsWith('.mp4'))[0]. If no file found, throw new ScrapeError('UNAVAILABLE', 'Could not download video file.').
6. filePath = path.join(getTempDir(videoId), fileName)
7. const duration = await getVideoDuration(filePath)
8. If duration > MAX_DURATION_SECONDS: throw new ScrapeError('TOO_LONG', `This video is ${Math.round(duration/60)} minutes long. Maximum is 20 minutes.`)
9. const sizeBytes = getFileSizeBytes(filePath)
10. If sizeBytes > MAX_FILE_SIZE_BYTES: await compressTo720p(filePath)
11. Return { filePath, duration, sizeBytes: getFileSizeBytes(filePath) }

In catch block: rethrow ScrapeErrors directly. For other errors: throw mapYtDlpError(error).

NOTE: Do NOT call cleanTempDir here — that is the worker handler's responsibility (in finally block). Download only — no cleanup.

**src/lib/scraper/transcript-extractor.ts** — VTT subtitle extraction.

Imports: { exec } from 'child_process', { promisify } from 'util', fs from 'fs', path from 'path'.
Import: { getTempDir } from './temp-files'.

const execAsync = promisify(exec);

Export `extractSubtitles(youtubeUrl: string, videoId: string): Promise<string | null>`:

1. const tmpDir = getTempDir(videoId) — directory already exists from download step
2. yt-dlp command to extract only subtitles (no video download):
```
yt-dlp --write-auto-sub --sub-lang en --skip-download --output "${tmpDir}/%(id)s" "${youtubeUrl}"
```
3. execAsync(cmd, { timeout: 60 * 1000 }) — 1 minute timeout for subtitle extraction
4. Look for .en.vtt file: fs.readdirSync(tmpDir).find(f => f.endsWith('.en.vtt'))
5. If found: return path.join(tmpDir, vttFile)
6. If not found: return null (no captions available — caller handles this gracefully)
7. In catch: log the error but return null (subtitle extraction failure is non-fatal — scrape continues without transcript)

**src/lib/scraper/vtt-parser.ts** — VTT → TranscriptSegment[] parser.

Import fs from 'fs'.

Export interface TranscriptSegment:
```typescript
export interface TranscriptSegment {
  timestamp: string;   // Human-readable: "0:42" or "1:23:45"
  startMs: number;     // Milliseconds from video start — for seek-on-click
  text: string;        // Clean plain text, no HTML tags
}
```

Export `parseVTT(filePath: string): TranscriptSegment[]`:

State machine parser:
1. const content = fs.readFileSync(filePath, 'utf-8')
2. Split on '\n' and iterate lines
3. Skip lines starting with 'WEBVTT', 'NOTE', 'STYLE', empty lines, numeric cue identifiers (lines that are just a number)
4. Timestamp line detection: line.includes(' --> '). Parse start timestamp from `line.split(' --> ')[0].trim()` — strip any cue settings (anything after the timestamp). E.g., "00:00:42.500 --> 00:00:45.000 align:start position:0%" → start = "00:00:42.500"
5. Text lines: strip HTML tags (regex `/<[^>]*>/g` → ''), trim, skip empty
6. On next timestamp line: save previous segment, start new one
7. Deduplicate: if a segment's text === previous segment's text, skip it (VTT auto-captions repeat each line as new words arrive)

Helper `vttTimeToMs(vttTime: string): number`:
- Input: "00:00:42.500" or "00:42.500"
- Split by ':'. If 3 parts: h*3600000 + m*60000 + s*1000. If 2 parts: m*60000 + s*1000.
- parseFloat on the seconds part

Helper `formatTimestamp(ms: number): string`:
- < 3600000 (under 1 hour): "M:SS" format (e.g., "0:42")
- >= 3600000: "H:MM:SS" format (e.g., "1:02:35")

Export `transcriptToPlainText(segments: TranscriptSegment[]): string`:
- Returns segments.map(s => s.text).join(' ')
- Used for storing plain text version in the database
  </action>
  <verify>
    <automated>cd /c/Antigravity/Sandbox && npx tsc --noEmit 2>&1 | head -20</automated>
  </verify>
  <done>
    TypeScript compiles cleanly. downloadYouTubeVideo exported with DownloadResult interface. extractSubtitles returns string | null. parseVTT exports TranscriptSegment interface and function. transcriptToPlainText exported. All files use only relative imports within src/lib/scraper/.
  </done>
</task>

</tasks>

<verification>
Run: npx tsc --noEmit — must pass with zero errors.
Check: grep -r "from '@/" src/lib/scraper/ — must return empty (no @/ aliases in scraper lib).
Check: src/lib/scraper/temp-files.ts contains "cleanTempDir" with "MANDATORY" JSDoc comment.
Check: src/lib/scraper/error-codes.ts exports ScrapeError class with code property.
Check: src/lib/scraper/vtt-parser.ts exports TranscriptSegment interface with timestamp, startMs, text fields.
</verification>

<success_criteria>
- downloadYouTubeVideo() wraps yt-dlp with 5-minute timeout
- compressTo720p() called when file exceeds 200MB
- TOO_LONG error thrown when video duration exceeds 20 minutes
- extractSubtitles() returns null gracefully when no captions found (non-fatal)
- parseVTT() deduplicates repeated lines from auto-captions
- cleanTempDir() swallows errors (safe to call even if dir doesn't exist)
- All error codes mapped to user-facing strings
- Zero @/ alias imports — all relative
</success_criteria>

<output>
After completion, create `.planning/phases/02-scraping-pipeline-4-hours/02-01-SUMMARY.md` following the summary template.
</output>
