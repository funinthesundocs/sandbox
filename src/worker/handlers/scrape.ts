// src/worker/handlers/scrape.ts
// BullMQ scrape job handler — orchestrates the full scrape pipeline.
//
// IMPORTANT: This file runs in the WORKER process (outside Next.js).
// ALL imports MUST be relative — no @/ aliases.
// Uses supabaseAdmin (service role) — bypasses RLS.
// cleanTempDir(videoId) MUST be in the finally block.

import { Job } from 'bullmq';
import fs from 'fs';
import { downloadYouTubeVideo } from '../../lib/scraper/youtube-downloader';
import { extractSubtitles } from '../../lib/scraper/transcript-extractor';
import { parseVTT, transcriptToPlainText, TranscriptSegment } from '../../lib/scraper/vtt-parser';
import { cleanTempDir } from '../../lib/scraper/temp-files';
import { ScrapeError } from '../../lib/scraper/error-codes';
import { fetchVideoMetadata } from '../../lib/youtube-api/metadata';
import { supabaseAdmin } from '../../lib/supabase/admin';
import { storagePath } from '../../lib/remix-engine/hooks';

export interface ScrapeJobData {
  youtubeUrl: string;
  youtubeId: string;
  projectId: string;
  videoId: string;    // UUID — pre-generated by API route when creating re_videos record
  userId: string;     // Created-by user ID
  batchJobId?: string; // Present for batch jobs
}

/**
 * handleScrapeJob — Full scrape pipeline orchestrator.
 *
 * Pipeline steps:
 *   0. Mark job as processing (progress 10)
 *   1. Download video + fetch metadata in parallel (progress 40)
 *   2. Extract subtitles (progress 60)
 *   3. Upload assets to Supabase Storage (progress 85)
 *   4. Update re_videos with all scraped data
 *   5. Mark re_jobs complete (progress 100)
 *
 * Always calls cleanTempDir in finally block regardless of success or failure.
 */
export async function handleScrapeJob(
  job: Job<ScrapeJobData>
): Promise<{ success: true; videoId: string }> {
  const { youtubeUrl, youtubeId, projectId, videoId } = job.data;

  // DRY helper: update progress in both BullMQ job and re_jobs DB record
  const updateProgress = async (pct: number) => {
    await job.updateProgress(pct);
    await supabaseAdmin
      .from('re_jobs')
      .update({ progress: pct })
      .eq('id', job.id);
  };

  try {
    // ----------------------------------------------------------------
    // Step 0 — Mark job + video as processing
    // ----------------------------------------------------------------
    await supabaseAdmin
      .from('re_jobs')
      .update({
        status: 'processing',
        started_at: new Date().toISOString(),
        progress: 10,
      })
      .eq('id', job.id);

    await supabaseAdmin
      .from('re_videos')
      .update({ scrape_status: 'processing' })
      .eq('id', videoId);

    await job.updateProgress(10);

    // ----------------------------------------------------------------
    // Step 1 — Download video + fetch metadata in parallel (both I/O-bound)
    // ----------------------------------------------------------------
    const [downloadResult, metadata] = await Promise.all([
      downloadYouTubeVideo(youtubeUrl, videoId),
      fetchVideoMetadata(youtubeId),
    ]);

    await updateProgress(40);

    // ----------------------------------------------------------------
    // Step 2 — Extract subtitles (sequential — needs the temp dir from download)
    // ----------------------------------------------------------------
    const vttPath = await extractSubtitles(youtubeUrl, videoId);

    let transcriptSegments: TranscriptSegment[] = [];
    let transcriptText = '';

    if (vttPath) {
      transcriptSegments = parseVTT(vttPath);
      transcriptText = transcriptToPlainText(transcriptSegments);
    }
    // If vttPath is null: transcriptSegments stays [], transcriptText stays ''
    // This is not an error — job continues with no transcript

    await updateProgress(60);

    // ----------------------------------------------------------------
    // Step 3 — Upload assets to Supabase Storage in parallel
    // ----------------------------------------------------------------
    const bucket = 'remix-engine';
    const videoPath = storagePath('videos', projectId, videoId, 'original.mp4');
    const thumbnailPath = storagePath('videos', projectId, videoId, 'thumbnail.jpg');
    const transcriptJsonPath = storagePath('videos', projectId, videoId, 'transcript.json');
    const transcriptTxtPath = storagePath('videos', projectId, videoId, 'transcript.txt');

    // Fetch thumbnail from YouTube CDN (public URL from metadata)
    const thumbnailArrayBuffer = await fetch(metadata.thumbnailUrl).then((r) => r.arrayBuffer());
    const thumbnailBuffer = Buffer.from(new Uint8Array(thumbnailArrayBuffer));

    const uploads = await Promise.all([
      supabaseAdmin.storage.from(bucket).upload(
        videoPath,
        fs.readFileSync(downloadResult.filePath),
        { contentType: 'video/mp4', upsert: true }
      ),
      supabaseAdmin.storage.from(bucket).upload(
        thumbnailPath,
        thumbnailBuffer,
        { contentType: 'image/jpeg', upsert: true }
      ),
      supabaseAdmin.storage.from(bucket).upload(
        transcriptJsonPath,
        Buffer.from(JSON.stringify(transcriptSegments)),
        { contentType: 'application/json', upsert: true }
      ),
      supabaseAdmin.storage.from(bucket).upload(
        transcriptTxtPath,
        Buffer.from(transcriptText),
        { contentType: 'text/plain', upsert: true }
      ),
    ]);

    const uploadErrors = uploads.filter((u) => u.error);
    if (uploadErrors.length > 0) {
      throw new ScrapeError('STORAGE_UPLOAD_FAILED', 'Failed to upload assets to storage.');
    }

    await updateProgress(85);

    // ----------------------------------------------------------------
    // Step 4 — Update re_videos record with all scraped data
    // ----------------------------------------------------------------
    const { error: videoError } = await supabaseAdmin
      .from('re_videos')
      .update({
        original_title: metadata.title,
        original_description: metadata.description,
        original_thumbnail_url: metadata.thumbnailUrl,
        original_transcript: transcriptText || null,
        channel_name: metadata.channelName,
        channel_id: metadata.channelId,
        duration_seconds: metadata.durationSeconds,
        view_count: metadata.viewCount,
        published_at: metadata.publishedAt,
        video_file_path: videoPath,
        thumbnail_file_path: thumbnailPath,
        transcript_file_path: transcriptJsonPath,
        scrape_status: 'complete',
        error_message: null,
      })
      .eq('id', videoId);

    if (videoError) {
      throw new ScrapeError(
        'UNKNOWN',
        'Failed to update video record after scrape.',
        videoError
      );
    }

    // ----------------------------------------------------------------
    // Step 5 — Mark re_jobs complete
    // ----------------------------------------------------------------
    await supabaseAdmin
      .from('re_jobs')
      .update({
        status: 'complete',
        progress: 100,
        completed_at: new Date().toISOString(),
        result: { videoId, transcriptSegmentCount: transcriptSegments.length },
      })
      .eq('id', job.id);

    await job.updateProgress(100);

    return { success: true, videoId };

  } catch (error: unknown) {
    // Map ScrapeError to user-facing message; otherwise generic message
    const userMessage =
      error instanceof ScrapeError
        ? error.userMessage
        : 'An unexpected error occurred during scraping.';

    await supabaseAdmin
      .from('re_jobs')
      .update({
        status: 'error',
        error_message: userMessage,
        completed_at: new Date().toISOString(),
      })
      .eq('id', job.id);

    await supabaseAdmin
      .from('re_videos')
      .update({
        scrape_status: 'error',
        error_message: userMessage,
      })
      .eq('id', videoId);

    throw error; // Re-throw so BullMQ marks job as failed

  } finally {
    // MANDATORY: always runs — cleans up /tmp/remixengine/{videoId}
    cleanTempDir(job.data.videoId);
  }
}
